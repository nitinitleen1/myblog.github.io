I"˜<p>Adding a new node to the MongoDB replica set with huge amount of data will take a lot of time to perform the initial sync. Recently I was working on a replica set where I need to replace all the nodes in the existing shard and add a new node to the shard‚Äôs replica set. The data size is 3TB on each shard. The entire infra is on GCP. I planned to do this activity with native approach, but later I realized it won‚Äôt work as I expected.</p>

<h2 id="the-native-approach">The native approach:</h2>

<p>In this approach, I just added the new node the replica set.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PRIMARY&gt; rs.add<span class="o">(</span><span class="s2">"new-node:27017"</span><span class="o">)</span>
</code></pre></div></div>

<p>After 24hrs, I checked the amount of data that has been synced. You know what, its just 100GB. Even my nodes are having 16 core CPU and better network bandwidth.</p>

<blockquote>
  <p>In GCP, we can get the network throughput upto 16Gbps and its depends on the CPU Core. One core supports 2Gbps. So I can get the maximum throughput with my current configuration.</p>
</blockquote>

<p>In AWS, its a different case, we have separate instance types.</p>

<p>MongoDB supports file system level backup to resync the replica set. Because Journal files will help to make the data is consistent.</p>

<blockquote>
  <h3 id="from-mongodb">FROM MONGODB:</h3>

  <p>Sync by Copying Data Files from Another Member</p>

  <p>This approach ‚Äúseeds‚Äù a new or stale member using the data files from an existing member of the replica set. The data files <strong>must</strong> be sufficiently recent to allow the new member to catch up with the <a href="https://docs.mongodb.com/manual/reference/glossary/#term-oplog">oplog</a>. Otherwise the member would need to perform an initial sync.</p>
</blockquote>

<blockquote>
  <p>Also make sure the journal files should be located on the same data disk.</p>
</blockquote>

<p>Yes, I have journaling enabled on all the mongodb nodes.</p>

<h2 id="rsync">Rsync:</h2>

<p>This time, I want use rsync command to manually sync the files from any one of the secondary server to the new node. I know it‚Äôll end up inconsistency files. But rsync knows which files got modified and we can run the rsync one more time. So it‚Äôll sync the modified files.</p>

<p>The plan was,</p>

<ol>
  <li>Run rsync from Secondary server to the new server.</li>
  <li>Once the 1st rsync has been done, then stop the mongodb service on Secondary node.</li>
  <li>Run the rsync again. This time it‚Äôll sync only the modified files.</li>
  <li>Start the mongodb service on the secondary once the rsync is done.</li>
  <li>Start the mongodb service on the new node.</li>
</ol>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@new-node# rsync <span class="nt">-Pavz</span> <span class="nt">-e</span> <span class="s2">"ssh -i key.pem"</span> bhuvi@10.10.10.10:/mongodb/data/ /mongodb/data/
</code></pre></div></div>

<p>This time, again the bandwidth problem. I got upto 200Mbps only. After 2.3TB it reduced to 10Mbps.</p>

<h3 id="gcs">GCS:</h3>

<p>This is same as the above method, but instead of sync the files between servers, this time sync to GCS using <code class="highlighter-rouge">gsutil</code> Its a command line to which supports rsync.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@new-node# gsutil <span class="nt">-m</span> rsync <span class="nt">-r</span> /mongodb/data/ gs://mongo-migration/secondary-node/
</code></pre></div></div>

<p>In this case, the data transfer is pretty fast. I got 500Mbps to 1Gbps. So the sync was done in few hours.</p>

<ul>
  <li>But few journal data files were not uploaded due to permission issue. Even I added the root user to mongod group.</li>
  <li>Some files were not uploaded, because that time those files are in use and was doing some changes.</li>
</ul>

<p>So inconsistent files which end up with corruption on the data files.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-08-14T13:09:56.407+0000 E STORAGE  [initandlisten] WiredTiger error (-31803) 
[1565788196:406419][28166:0x7feda45c4600], file:WiredTiger.wt, 
WT_CURSOR.next: __schema_create_collapse, 
111: metadata information for source configuration "colgroup:collection-29962-5646082836428872281" not found: 
WT_NOTFOUND: item not found
</code></pre></div></div>

<h2 id="snapshots">Snapshots:</h2>

<p>If you are familiar with public cloud platform then you already know that Snapshots are not consistent backups. I have already faced some issues with this snapshots when I used it for PostgreSQL. (<a href="https://thedataguy.in/dont-use-aws-ami-backup-ec2-database-server/">Here is a blog about that</a>)</p>

<blockquote>
  <p><strong>NOTE</strong> Once the snapshot restored, it‚Äôll not give the complete performance, it‚Äôs need some time to restore all the blocks. But yes, you can add it as a secondary node, then later you can promote this node as Master.</p>
</blockquote>

<p>But in mongodb, its a different case, Journal files will make your data consistent. So lets give a try.</p>

<ol>
  <li>Take snapshot of the data volume from the existing secondary server.</li>
  <li>Create a volume from the snapshot.</li>
  <li>Attach this volume to the New node.</li>
  <li>Mount it on the <code class="highlighter-rouge">dbpath</code> and assign the permissions.</li>
  <li>Make sure the <code class="highlighter-rouge">mongod.conf</code> has replica set and shard details.</li>
  <li>Start the mongodb on the new node.</li>
  <li>Add the new node to the replica set.</li>
</ol>

<p>In my case, I used CentOS for the new node. Here are the list of commands we need to use.</p>

<h4 id="on-the-new-node">On the new node:</h4>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>service mongod stop
mount /dev/sdb /mongodb/data/  <span class="o">(</span>replace the /dev/sdb with your block device and mount point as your dbpath<span class="o">)</span>
<span class="nb">chown</span> <span class="nt">-R</span> mongod:mongod /mongodb/data/ 
<span class="nb">chcon </span>system_u:object_r:mongod_var_lib_t:s0 /mongodb/data 
restorecon /mongodb/data
</code></pre></div></div>

<p>If you are using mongodb with Key file auth for replication, then assign the below permissions for the key file.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chown </span>400 mongodb-keyfile 
<span class="nb">chown </span>mongod:mongod mongodb-keyfile 
<span class="nb">chcon </span>system_u:object_r:mongod_var_lib_t:s0 mongodb-keyfile
service mongod start
</code></pre></div></div>

<h4 id="on-primary">On Primary:</h4>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PRIMARY&gt; rs.add<span class="o">(</span><span class="s2">"new-node:27017"</span><span class="o">)</span>
</code></pre></div></div>

<p>It‚Äôll take some time to catch up the lag. Then we are ready to use the new node.</p>

<p>I remember 2 years before, I have done this on AWS. <a href="https://dba.stackexchange.com/a/164391/105575">https://dba.stackexchange.com/a/164391/105575</a></p>
:ET
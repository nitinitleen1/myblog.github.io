I"ç<p>We are living in the DataLake world. Now almost every organizations wants their reporting in Near Real Time. Kafka is of the best streaming platform for realtime reporting. Based on the Kafka connector, RedHat designed the Debezium which is an OpenSource product and high recommended for real time CDC from transnational databases. I referred many blogs to setup this cluster. But I found just basic installation steps. So I setup this cluster for AWS with Production grade and publishing this blog.</p>

<h2 id="a-shot-intro">A shot intro:</h2>

<p>Debezium is a set of distributed services to capture changes in your databases so that your applications can see those changes and respond to them. Debezium records all row-level changes within each database table in a <em>change event stream</em>, and applications simply read these streams to see the change events in the same order in which they occurred.</p>

<h2 id="basic-tech-terms">Basic Tech Terms:</h2>

<ul>
  <li><strong>Kafka Broker:</strong> Brokers are the core for the kafka streaming, they‚Äôll keep your messages and giving it to the consumers.</li>
  <li><strong>Zookeeper</strong>: It‚Äôll maintain the cluster status and node status. It‚Äôll help to make the Kafka‚Äôs availability.</li>
  <li><strong>Producers:</strong> The component who will send the messages(data) to the Broker.</li>
  <li><strong>Consumers:</strong> The component who will get the messages from the Queue for further analytics.</li>
  <li><strong>Confluent:</strong> Confluent is having their own steaming platform which basically using Apache Kafka under the hood. But it has more features.</li>
</ul>

<p>Here <strong>Debezium</strong> is our data producer and <strong>S3sink</strong> is our consumer. For this setup, Im going to stream the MySQL data changes to S3 with customized format.</p>

<h2 id="aws-architecture">AWS Architecture:</h2>

<p><img src="/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-3.jpg" alt="" /></p>

<p>Kafka and Zookeepers are installed on the same EC2. We we‚Äôll deploy 3 node confluent Kafka cluster. Each node will be in a different availability zone.</p>

<ul>
  <li>172.31.47.152 - Zone A</li>
  <li>172.31.38.158 - Zone B</li>
  <li>172.31.46.207 - Zone C</li>
</ul>

<p>For Producer(debezium) and Consumer(S3sink) will be hosted on the same Ec2. We‚Äôll 3 nodes for this.</p>

<ul>
  <li>172.31.47.12 - Zone A</li>
  <li>172.31.38.183 - Zone B</li>
  <li>172.31.46.136 - Zone C</li>
</ul>

<h2 id="instance-type">Instance Type:</h2>

<p>Kafka nodes are generally needs Memory and Network Optimized. You can choose either Persistent and ephemeral storage. I prefer Persistent SSD Disks for Kafka storage. So add n GB size disk to your Kafka broker nodes. For Normal work loads its better to go with R5 instance Family.</p>

<p>Mount the Volume in <code class="highlighter-rouge">/kafkadata</code> location.</p>

<h2 id="security-group">Security Group:</h2>

<p>Use a new Security group which allows the below ports.</p>

<p><img src="/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-4.jpg" alt="" /></p>

<h2 id="installation">Installation:</h2>

<p>Install the Java and Kafka on all the Broker nodes.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span> Install OpenJDK
apt-get <span class="nt">-y</span> update 
<span class="nb">sudo </span>apt-get <span class="nt">-y</span> <span class="nb">install </span>default-jre

<span class="nt">--</span> Install Confluent Kafka platform
wget <span class="nt">-qO</span> - https://packages.confluent.io/deb/5.3/archive.key | <span class="nb">sudo </span>apt-key add -
<span class="nb">sudo </span>add-apt-repository <span class="s2">"deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main"</span>
<span class="nb">sudo </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get <span class="nb">install </span>confluent-platform-2.12</code></pre></figure>

<h2 id="configuration">Configuration:</h2>

<p>We need to configure Zookeeper and Kafaka properties, Edit the <code class="highlighter-rouge">/etc/kafka/zookeeper.properties</code> on all the kafka nodes</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span> On Node 1
<span class="nv">dataDir</span><span class="o">=</span>/var/lib/zookeeper
<span class="nv">clientPort</span><span class="o">=</span>2181
<span class="nv">maxClientCnxns</span><span class="o">=</span>0
server.1<span class="o">=</span>0.0.0.0:2888:3888
server.2<span class="o">=</span>172.31.38.158:2888:3888
server.3<span class="o">=</span>172.31.46.207:2888:3888
autopurge.snapRetainCount<span class="o">=</span>3
autopurge.purgeInterval<span class="o">=</span>24
<span class="nv">initLimit</span><span class="o">=</span>5
<span class="nv">syncLimit</span><span class="o">=</span>2

<span class="nt">--</span> On Node 2
<span class="nv">dataDir</span><span class="o">=</span>/var/lib/zookeeper
<span class="nv">clientPort</span><span class="o">=</span>2181
<span class="nv">maxClientCnxns</span><span class="o">=</span>0
server.1<span class="o">=</span>172.31.47.152:2888:3888
server.2<span class="o">=</span>0.0.0.0:2888:3888
server.3<span class="o">=</span>172.31.46.207:2888:3888
autopurge.snapRetainCount<span class="o">=</span>3
autopurge.purgeInterval<span class="o">=</span>24
<span class="nv">initLimit</span><span class="o">=</span>5
<span class="nv">syncLimit</span><span class="o">=</span>2

<span class="nt">--</span> On Node 3
<span class="nv">dataDir</span><span class="o">=</span>/var/lib/zookeeper
<span class="nv">clientPort</span><span class="o">=</span>2181
<span class="nv">maxClientCnxns</span><span class="o">=</span>0
server.1<span class="o">=</span>172.31.47.152:2888:3888
server.2<span class="o">=</span>172.31.38.158:2888:3888
server.3<span class="o">=</span>0.0.0.0:2888:3888
autopurge.snapRetainCount<span class="o">=</span>3
autopurge.purgeInterval<span class="o">=</span>24
<span class="nv">initLimit</span><span class="o">=</span>5
<span class="nv">syncLimit</span><span class="o">=</span>2</code></pre></figure>

<p>We need to assign a unique ID for all the Zookeeper nodes.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"> <span class="nt">--</span> On Node 1
 <span class="nb">echo</span> <span class="s2">"1"</span> <span class="o">&gt;</span> /var/lib/zookeeper/myid
 
 <span class="nt">--On</span> Node 2
 <span class="nb">echo</span> <span class="s2">"2"</span> <span class="o">&gt;</span> /var/lib/zookeeper/myid
 
 <span class="nt">--On</span> Node 3
 <span class="nb">echo</span> <span class="s2">"3"</span> <span class="o">&gt;</span> /var/lib/zookeeper/myid</code></pre></figure>

<p>Now we need to configure Kafka broker. So edit the <code class="highlighter-rouge">/etc/kafka/server.properties</code> on all the kafka nodes.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--On</span> Node 1
broker.id.generation.enable<span class="o">=</span><span class="nb">true
</span>delete.topic.enable<span class="o">=</span><span class="nb">true
</span><span class="nv">listeners</span><span class="o">=</span>PLAINTEXT://:9092
zookeeper.connect<span class="o">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181
log.dirs<span class="o">=</span>/kafkadata/kafka
log.retention.hours<span class="o">=</span>168
num.partitions<span class="o">=</span>1

<span class="nt">--On</span> Node 2
broker.id.generation.enable<span class="o">=</span><span class="nb">true
</span>delete.topic.enable<span class="o">=</span><span class="nb">true
</span><span class="nv">listeners</span><span class="o">=</span>PLAINTEXT://:9092
log.dirs<span class="o">=</span>/kafkadata/kafka
zookeeper.connect<span class="o">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181
log.retention.hours<span class="o">=</span>168
num.partitions<span class="o">=</span>1

<span class="nt">--</span> On Node 3
broker.id.generation.enable<span class="o">=</span><span class="nb">true
</span>delete.topic.enable<span class="o">=</span><span class="nb">true
</span><span class="nv">listeners</span><span class="o">=</span>PLAINTEXT://:9092
log.dirs<span class="o">=</span>/kafkadata/kafka
zookeeper.connect<span class="o">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181
num.partitions<span class="o">=</span>1
log.retention.hours<span class="o">=</span>168</code></pre></figure>

<p>The next step is optimizing the <code class="highlighter-rouge">Java JVM Heap</code> size, In many places kafka will go down due to the less heap size. So Im allocating 50% of the Memory to Heap. But make sure more Heap size also bad. Please refer some documentation to set this value for very heavy systems.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vi /usr/bin/kafka-server-start
<span class="nb">export </span><span class="nv">KAFKA_HEAP_OPTS</span><span class="o">=</span><span class="s2">"-Xmx2G -Xms2G"</span></code></pre></figure>

<p>The another major problem in the kafka system is the open file descriptors. So we need to allow the kafka to open at least up to 100000 files.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vi /etc/pam.d/common-session
session required pam_limits.so

vi /etc/security/limits.conf

<span class="k">*</span>                       soft    nofile          10000
<span class="k">*</span>                       hard    nofile          100000
cp-kafka                soft    nofile          10000
cp-kafka                hard    nofile          100000</code></pre></figure>

<p>Here the <code class="highlighter-rouge">cp-kafka</code> is the default user for the kafka process.</p>

<h3 id="create-kafka-data-dir">Create Kafka data dir:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">mkdir</span> <span class="nt">-p</span> /kafkadata/kafka
<span class="nb">chown</span> <span class="nt">-R</span> cp-kafka:confluent /kafkadata/kafka
chmode 710 /kafkadata/kafka</code></pre></figure>

<h3 id="start-the-kafka-cluster">Start the Kafka cluster:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>systemctl start confluent-zookeeper
<span class="nb">sudo </span>systemctl start confluent-kafka
<span class="nb">sudo </span>systemctl start confluent-schema-registry</code></pre></figure>

<p>Make sure the Kafka has to automatically starts after the Ec2 restart.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>systemctl <span class="nb">enable </span>confluent-zookeeper
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>confluent-kafka
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>confluent-schema-registry</code></pre></figure>

<p>Now our kafka cluster is ready. To check the list of system topics run the following command.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kafka-topics <span class="nt">--list</span> <span class="nt">--zookeeper</span> localhost:2181

__confluent.support.metrics</code></pre></figure>

<h2 id="setup-debezium">Setup Debezium:</h2>

<p>Install the confluent connector and debezium MySQL connector on all the producer nodes.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">apt-get update 
<span class="nb">sudo </span>apt-get <span class="nb">install </span>default-jre
 
wget <span class="nt">-qO</span> - https://packages.confluent.io/deb/5.3/archive.key | <span class="nb">sudo </span>apt-key add -
<span class="nb">sudo </span>add-apt-repository <span class="s2">"deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main"</span>
<span class="nb">sudo </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get <span class="nb">install </span>confluent-hub-client confluent-common confluent-kafka-connect-s3 confluent-kafka-2.12</code></pre></figure>

<h3 id="configuration-1">Configuration:</h3>

<p>Edit the <code class="highlighter-rouge">/etc/kafka/connect-distributed.properties</code> on all the producer nodes to make our producer will run on a distributed manner.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span> On all the connector nodes
bootstrap.servers<span class="o">=</span>172.31.47.152:9092,172.31.38.158:9092,172.31.46.207:9092
group.id<span class="o">=</span>debezium-cluster
plugin.path<span class="o">=</span>/usr/share/java,/usr/share/confluent-hub-components</code></pre></figure>

<h3 id="install-debezium-mysql-connector">Install Debezium MySQL Connector:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">confluent-hub <span class="nb">install </span>debezium/debezium-connector-mysql:latest</code></pre></figure>

<p>it‚Äôll ask for making some changes just select <code class="highlighter-rouge">Y</code> for everything.</p>

<h3 id="run-the-distributed-connector-as-a-service">Run the distributed connector as a service:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vi /lib/systemd/system/confluent-connect-distributed.service

<span class="o">[</span>Unit]
<span class="nv">Description</span><span class="o">=</span>Apache Kafka - connect-distributed
<span class="nv">Documentation</span><span class="o">=</span>http://docs.confluent.io/
<span class="nv">After</span><span class="o">=</span>network.target

<span class="o">[</span>Service]
<span class="nv">Type</span><span class="o">=</span>simple
<span class="nv">User</span><span class="o">=</span>cp-kafka
<span class="nv">Group</span><span class="o">=</span>confluent
<span class="nv">ExecStart</span><span class="o">=</span>/usr/bin/connect-distributed /etc/kafka/connect-distributed.properties
<span class="nv">TimeoutStopSec</span><span class="o">=</span>180
<span class="nv">Restart</span><span class="o">=</span>no

<span class="o">[</span>Install]
<span class="nv">WantedBy</span><span class="o">=</span>multi-user.target</code></pre></figure>

<h3 id="start-the-service">Start the Service:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">systemctl <span class="nb">enable </span>confluent-connect-distributed
systemctl start confluent-connect-distributed</code></pre></figure>

<h2 id="configure-debezium-mysql-connector">Configure Debezium MySQL Connector:</h2>

<p>Create a <code class="highlighter-rouge">mysql.json</code> file which contains the MySQL information and other formatting options.</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
	</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mysql-connector-db01"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mysql-connector-db01"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"connector.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.debezium.connector.mysql.MySqlConnector"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.server.id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"tasks.max"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.history.kafka.bootstrap.servers"</span><span class="p">:</span><span class="w"> </span><span class="s2">"172.31.47.152:9092,172.31.38.158:9092,172.31.46.207:9092"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.history.kafka.topic"</span><span class="p">:</span><span class="w"> </span><span class="s2">"schema-changes.mysql"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.server.name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mysql-db01"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.hostname"</span><span class="p">:</span><span class="w"> </span><span class="s2">"172.31.84.129"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.port"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3306"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.user"</span><span class="p">:</span><span class="w"> </span><span class="s2">"bhuvi"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.password"</span><span class="p">:</span><span class="w"> </span><span class="s2">"my_stong_password"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.whitelist"</span><span class="p">:</span><span class="w"> </span><span class="s2">"proddb,test"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.key.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"key.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.key.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.value.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"value.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.value.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"value.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"key.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"transforms"</span><span class="p">:</span><span class="w"> </span><span class="s2">"unwrap"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"transforms.unwrap.type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.debezium.transforms.ExtractNewRecordState"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"transforms.unwrap.add.source.fields"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ts_ms"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"tombstones.on.delete"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
	</span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<ul>
  <li>‚Äúdatabase.history.kafka.bootstrap.servers‚Äù - Kafka Servers IP.</li>
  <li>‚Äúdatabase.whitelist‚Äù - List of databases to get the CDC.</li>
  <li>key.converter and value.converter and transforms parameters - By default Debezium output will have more detailed information. But I don‚Äôt want all of those information. Im only interested in to get the new row and the timestamp when its inserted.</li>
</ul>

<p>If you don‚Äôt want to customize anythings then just remove everything after the <code class="highlighter-rouge">database.whitelist</code></p>

<h3 id="register-the-mysql-connector">Register the MySQL Connector:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl <span class="nt">-X</span> POST <span class="nt">-H</span> <span class="s2">"Accept: application/json"</span> <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> http://localhost:8083/connectors <span class="nt">-d</span> @mysql.json</code></pre></figure>

<h3 id="check-the-status">Check the status:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl GET localhost:8083/connectors/mysql-connector-db01/status
<span class="o">{</span>
  <span class="s2">"name"</span>: <span class="s2">"mysql-connector-db01"</span>,
  <span class="s2">"connector"</span>: <span class="o">{</span>
    <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
    <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
  <span class="o">}</span>,
  <span class="s2">"tasks"</span>: <span class="o">[</span>
    <span class="o">{</span>
      <span class="s2">"id"</span>: 0,
      <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
      <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
    <span class="o">}</span>
  <span class="o">]</span>,
  <span class="s2">"type"</span>: <span class="s2">"source"</span>
<span class="o">}</span></code></pre></figure>

<h3 id="test-the-mysql-consumer">Test the MySQL Consumer:</h3>

<p>Now insert something into any tables in <code class="highlighter-rouge">proddb or test</code> (because we have whilelisted only these databaes to capture the CDC.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="n">use</span> <span class="n">test</span><span class="p">;</span>
<span class="k">create</span> <span class="k">table</span> <span class="n">rohi</span> <span class="p">(</span><span class="n">id</span> <span class="nb">int</span><span class="p">,</span>
<span class="n">fn</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
<span class="n">ln</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
<span class="n">phone</span> <span class="nb">int</span> <span class="p">);</span>

<span class="k">insert</span> <span class="k">into</span> <span class="n">rohi</span> <span class="k">values</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s1">'rohit'</span><span class="p">,</span> <span class="s1">'ayare'</span><span class="p">,</span><span class="s1">'87611'</span><span class="p">);</span></code></pre></figure>

<p>We can get these values from the Kafker brokers. Open any one the kafka node and run the below command.</p>

<p>I prefer confluent cli for this. By default it‚Äôll not be available, so download manually.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl <span class="nt">-L</span> https://cnfl.io/cli | sh <span class="nt">-s</span> <span class="nt">--</span> <span class="nt">-b</span> /usr/bin/</code></pre></figure>

<h3 id="listen-the-below-topic">Listen the below topic:</h3>

<blockquote>
  <p><strong>mysql-db01.test.rohi</strong> <br />
This is the combination of <code class="highlighter-rouge">servername.databasename.tablename</code> <br />
servername(you mentioned this in as a server name in mysql json file).</p>
</blockquote>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">confluent <span class="nb">local </span>consume mysql-db01.test.rohi

<span class="nt">----</span>
The <span class="nb">local </span>commands are intended <span class="k">for </span>a single-node development environment
only, NOT <span class="k">for </span>production usage. https://docs.confluent.io/current/cli/index.html
<span class="nt">-----</span>

<span class="o">{</span><span class="s2">"id"</span>:1,<span class="s2">"fn"</span>:<span class="s2">"rohit"</span>,<span class="s2">"ln"</span>:<span class="s2">"ayare"</span>,<span class="s2">"phone"</span>:87611,<span class="s2">"__ts_ms"</span>:1576757407000<span class="o">}</span></code></pre></figure>

<h2 id="setup-s3-sink-connector-in-all-producer-nodes">Setup S3 Sink connector in All Producer Nodes:</h2>

<p>I want to send this data to S3 bucket. So you must have an EC2 IAM role which has access to the target S3 bucket. Or install <code class="highlighter-rouge">awscli</code> and configure access and secret key(but its not recommended)</p>

<h3 id="install-s3-connector">Install S3 Connector:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">confluent-hub <span class="nb">install </span>confluentinc/kafka-connect-s3:latest</code></pre></figure>

<p>Create <code class="highlighter-rouge">s3.json</code> file.</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
	</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3-sink-db01"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"connector.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.confluent.connect.s3.S3SinkConnector"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"storage.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.confluent.connect.s3.storage.S3Storage"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"s3.bucket.name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"bhuvi-datalake"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3-sink-db01"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"tasks.max"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"s3.region"</span><span class="p">:</span><span class="w"> </span><span class="s2">"us-east-1"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"s3.part.size"</span><span class="p">:</span><span class="w"> </span><span class="s2">"5242880"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"s3.compression.type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"gzip"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"timezone"</span><span class="p">:</span><span class="w"> </span><span class="s2">"UTC"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"locale"</span><span class="p">:</span><span class="w"> </span><span class="s2">"en"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"flush.size"</span><span class="p">:</span><span class="w"> </span><span class="s2">"10000"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"rotate.interval.ms"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3600000"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"topics.regex"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mysql-db01.(.*)"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.key.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"key.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.key.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"format.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.confluent.connect.s3.format.json.JsonFormat"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.value.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"value.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.value.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"value.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"key.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"partitioner.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.confluent.connect.storage.partitioner.HourlyPartitioner"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"path.format"</span><span class="p">:</span><span class="w"> </span><span class="s2">"YYYY/MM/dd/HH"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"partition.duration.ms"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3600000"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"rotate.schedule.interval.ms"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3600000"</span><span class="w">
	</span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<ul>
  <li><code class="highlighter-rouge">"topics.regex": "mysql-db01"</code> - It‚Äôll send the data only from the topics which has <code class="highlighter-rouge">mysql-db01</code> as prefix. In our case all the MySQL databases related topics will start  with this prefix.</li>
  <li><code class="highlighter-rouge">"flush.size"</code> - The data will uploaded to S3 only after these many number of records stored. Or after <code class="highlighter-rouge">"rotate.schedule.interval.ms"</code> this duration.</li>
</ul>

<h3 id="register-this-s3-sink-connector">Register this S3 sink connector:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl <span class="nt">-X</span> POST <span class="nt">-H</span> <span class="s2">"Accept: application/json"</span> <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> http://localhost:8083/connectors <span class="nt">-d</span> @s3</code></pre></figure>

<h3 id="check-the-status-1">Check the Status:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl GET localhost:8083/connectors/s3-sink-db01/status
<span class="o">{</span>
  <span class="s2">"name"</span>: <span class="s2">"s3-sink-db01"</span>,
  <span class="s2">"connector"</span>: <span class="o">{</span>
    <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
    <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
  <span class="o">}</span>,
  <span class="s2">"tasks"</span>: <span class="o">[</span>
    <span class="o">{</span>
      <span class="s2">"id"</span>: 0,
      <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
      <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
    <span class="o">}</span>,
    <span class="o">{</span>
      <span class="s2">"id"</span>: 1,
      <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
      <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
    <span class="o">}</span>,
    <span class="o">{</span>
      <span class="s2">"id"</span>: 2,
      <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
      <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
    <span class="o">}</span>
  <span class="o">]</span>,
  <span class="s2">"type"</span>: <span class="s2">"sink"</span>
<span class="o">}</span></code></pre></figure>

<h3 id="test-the-s3-sync">Test the S3 sync:</h3>

<p>Insert the 10000 rows into the <code class="highlighter-rouge">rohi</code> table. Then check the S3 bucket. It‚Äôll save the data in JSON format with GZIP compression. Also in a HOUR wise partitions.</p>

<p><img src="/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-1.jpg" alt="" /></p>

<p><img src="/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-2.jpg" alt="" /></p>

<h2 id="monitoring">Monitoring:</h2>
<p>Refer <a href="https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/">this post</a> to setup monitoring for MySQL Connector.</p>

<h2 id="more-tuning">More Tuning:</h2>

<ul>
  <li>Replication Factor is the other main parameter to the data durability.</li>
  <li>Use internal IP addresses as much as you can.</li>
  <li>By default debezium uses 1 Partition per topic. You can configure this based on your work load. But more partitions more through put needed.</li>
</ul>

<h2 id="references">References:</h2>

<ol>
  <li><a href="https://docs.confluent.io/current/kafka/deployment.html">Setup Kafka in production by confluent</a></li>
  <li><a href="https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/">How to choose number of partition</a></li>
  <li><a href="https://log-it.ro/2017/10/16/ubuntu-change-ulimit-kafka-not-ignore/">Open file descriptors for Kafka </a></li>
  <li><a href="https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-kafka-on-aws/">Kafka best practices in AWS</a></li>
  <li><a href="https://debezium.io/documentation/reference/1.0/tutorial.html">Debezium documentation</a></li>
  <li><a href="https://debezium.io/documentation/reference/1.0/configuration/event-flattening.html">Customize debezium output with SMT</a></li>
</ol>

<h3 id="debezium-series-blogs">Debezium Series blogs:</h3>

<ol>
  <li><a href="https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/">Build Production Grade Debezium Cluster With Confluent Kafka</a></li>
  <li><a href="https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/">Monitor Debezium MySQL Connector With Prometheus And Grafana</a></li>
  <li><a href="https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/">Debezium MySQL Snapshot From Read Replica With GTID</a></li>
  <li><a href="https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/">Debezium MySQL Snapshot From Read Replica And Resume From Master</a></li>
  <li><a href="https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot</a></li>
  <li><a href="https://medium.com/searce/realtime-cdc-from-mysql-using-aws-msk-with-debezium-28da5a4ca873">RealTime CDC From MySQL Using AWS MSK With Debezium</a></li>
</ol>
:ET
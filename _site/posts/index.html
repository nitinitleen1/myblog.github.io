<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.1.1
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="http://localhost:4000">
<script data-ad-client="ca-pub-9518508677792760" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<link rel="apple-touch-icon" sizes="57x57" href="/favicon.ico/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/favicon.ico/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/favicon.ico/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/favicon.ico/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/favicon.ico/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/favicon.ico/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/favicon.ico/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/favicon.ico/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="/favicon.ico/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon.ico/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico/favicon-16x16.png">
<link rel="manifest" href="/favicon.ico/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">  
  

  
    <title>Post Archive</title>
    <meta name="description" content="Mi occupo di web disign, grafica, web writing e sviluppo web.">
    <link rel="canonical" href="http://localhost:4000/posts/">
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  <link rel="stylesheet" href="/assets/css/applause-button.css" />
   <script src="/assets/js/applause-button.js"></script>

</head>


  <body class="layout--posts  post-archive">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/">Home</a></li><li><a href="/posts/">Posts</a></li><li><a href="/categories/">Categories</a></li><li><a href="/tags/">Tags</a></li><li><a href="https://medium.com/@bhuvithedataguy">BigData Articles</a></li><li><a href="/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">Nitin Kumar Singh | Data Architect</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">Mi occupo di web disign, grafica, web writing e sviluppo web.</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article>
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title">Post Archive</h1>
        
      </header>
      <div class="page-content">
        


<ul class="taxonomy-index">
  
  
    <li>
      <a href="#2020">
        <strong>2020</strong> <span class="taxonomy-count">1</span>
      </a>
    </li>
  
    <li>
      <a href="#2019">
        <strong>2019</strong> <span class="taxonomy-count">26</span>
      </a>
    </li>
  
    <li>
      <a href="#2018">
        <strong>2018</strong> <span class="taxonomy-count">17</span>
      </a>
    </li>
  
    <li>
      <a href="#2017">
        <strong>2017</strong> <span class="taxonomy-count">10</span>
      </a>
    </li>
  
</ul>



  <section id="2020" class="taxonomy-section">
    <h2 class="taxonomy-title">2020</h2>
    <div class="entries-list">
      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/" rel="bookmark">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>I have published enough Debezium MySQL connector tutorials for taking snapshots from Read Replica. To continue my research I wanted to do something for AWS RDS Aurora as well. But aurora is not using binlog bases replication. So we can’t use the list of tutorials that I published already. In Aurora, we can get the binlog file name and its position from its snapshot of the source Cluster. So I used a snapshot for loading the historical data, and once it’s loaded we can resume the CDC from the main cluster.</p>

        <div class="more-link">
          <a href="/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2020-01-02T14:13:00+05:30">January 2, 2020
  </time>
    </footer>
  
</article>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
  </section>

  <section id="2019" class="taxonomy-section">
    <h2 class="taxonomy-title">2019</h2>
    <div class="entries-list">
      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/" rel="bookmark">Debezium MySQL Snapshot From Read Replica And Resume From Master
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>In my <a href="https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/">previous post</a>, I have shown you how to take the snapshot from Read Replica with GTID for Debezium  MySQL connector. GTID concept is awesome, but still many of us using the replication without GTID. For these cases, we can take a snapshot from Read replica and then manually push the Master binlog information to the offsets topic. Injecting manual entry for offsets topic is <a href="https://debezium.io/documentation/faq/#how_to_change_the_offsets_of_the_source_database">already documented in Debezium</a>. I’m just guiding you the way to take snapshot from Read replica without GTID.</p>

        <div class="more-link">
          <a href="/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-12-31T17:40:00+05:30">December 31, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/debezium-mysql-snapshot-from-read-replica-with-gtid/" rel="bookmark">Debezium MySQL Snapshot From Read Replica With GTID
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>When you installed the Debezium MySQL connector, then it’ll start read your historical data and push all of them into the Kafka topics. This setting can we changed via <code class="highlighter-rouge">snapshot.mode</code> parameter in the connector. But if you are going to start a new sync, then Debezium will load the existing data its called Snapshot. Unfortunately, if you have a busy transactional MySQL database, then it may lead to some performance issues. And your DBA will never agree to read the data from Master Node.[Disclaimer: I’m a DBA :) ]. So I was thinking of figuring out to take the snapshot from the Read Replica, once the snapshot is done, then start read the realtime data from the Master. I found this useful information in a StackOverflow answer.</p>

        <div class="more-link">
          <a href="/debezium-mysql-snapshot-from-read-replica-with-gtid/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-12-28T16:47:00+05:30">December 28, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/redshift-kill-all-locking-sessions-on-a-table/" rel="bookmark">RedShift Kill All Locking Sessions On A Table
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>In any relational database, if you didn’t close the session properly, then it’ll lock your DDL queries. It’s applicable to RedShift as well. A few days back I got a scenario that we have to run some DROP TABLE commands to create some lookup tables. But every time while triggering this DDL it got stuck. Then we realize there were some sessions that are still open and those sessions are causing this locking. There we 30+ sessions. I know we can fix this by properly closing the session from the application side. But in some emergency cases, we need to kill all open sessions or locking session in Redshift.</p>

        <div class="more-link">
          <a href="/redshift-kill-all-locking-sessions-on-a-table/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-12-26T05:00:00+05:30">December 26, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/monitor-debezium-mysql-connector-with-prometheus-and-grafana/" rel="bookmark">Monitor Debezium MySQL Connector With Prometheus And Grafana
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Debezium is providing out of the box CDC solution from various databases. In my last blog post, I have published how to configure the Debezium MySQL connector. This is the next part of that post. Once we deployed the debezium, to we need some kind of monitoring to keep track of whats happening in the debezium connector. Luckily Debezium has its own metrics that are already integrated with the connectors. We just need to capture them using the JMX exporter agent. Here I have written how to monitor Debezium MySQL connector with Prometheus and Grafana. But the dashboard is having the basic metrics only. You can build your own dashboard for more detailed monitoring.</p>

        <div class="more-link">
          <a href="/monitor-debezium-mysql-connector-with-prometheus-and-grafana/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-12-24T11:50:00+05:30">December 24, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/build-production-grade-debezium-with-confluent-kafka-cluster/" rel="bookmark">Build Production Grade Debezium Cluster With Confluent Kafka
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>We are living in the DataLake world. Now almost every organizations wants their reporting in Near Real Time. Kafka is of the best streaming platform for realtime reporting. Based on the Kafka connector, RedHat designed the Debezium which is an OpenSource product and high recommended for real time CDC from transnational databases. I referred many blogs to setup this cluster. But I found just basic installation steps. So I setup this cluster for AWS with Production grade and publishing this blog.</p>

        <div class="more-link">
          <a href="/build-production-grade-debezium-with-confluent-kafka-cluster/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-12-19T21:13:00+05:30">December 19, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/redshift-unload-multiple-tables-schema-to-s3/" rel="bookmark">RedShift Unload Like A Pro - Multiple Tables And Schemas
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>In my <a href="https://thedataguy.in/redshift-unload-all-tables-to-s3/">previous post</a>, I explained how to unload all the tables in the RedShift database to S3 Bucket. But there was a limitation. We should export all the tables, you can’t specify some list of tables or all the tables in a specific schema. Its because of I can’t give comma separated values in RedShift stored procedure. But after spending few days I found a solution for this.</p>

        <div class="more-link">
          <a href="/redshift-unload-multiple-tables-schema-to-s3/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-11-22T10:05:00+05:30">November 22, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/redshift-stored-procedure-comma-separated-string-in-argument/" rel="bookmark">Redshift Stored Procedure Comma separated string in Argument
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>RedShift stored procedures are really useful feature and after a long time they introduced it. I tried this stored procedure for many use cases and recently I came up with a complexity in it. You can’t get a comma separated sting in the argument. Lets say I have an employee table and I want to pass the employee names in a variable/argument and you need to <code class="highlighter-rouge">select * from table where employee_name in ('your comma separated values)</code>. It’ll not work directly.</p>

        <div class="more-link">
          <a href="/redshift-stored-procedure-comma-separated-string-in-argument/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-11-07T14:30:00+05:30">November 7, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/where-gcp-internal-load-balancer-fails/" rel="bookmark">Where GCP Internal TCP Load Balancer Fails
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>GCP’s Load balancers are globally scalable and its the unique identify for GCP while comparing its competitors. Generally GCP’s networking is very strong and mature than other Cloud providers.  Recently I was working with a SQL Server setup which integrates the GCP Internal TCP load balancer. During that PoC setup I found this strange behaviour and then I ignored this problem because I thought its something related to SQL server.</p>

        <div class="more-link">
          <a href="/where-gcp-internal-load-balancer-fails/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-10-29T13:30:00+05:30">October 29, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/mysql-calculate-how-much-disk-space-you-wasted/" rel="bookmark">MySQL Calculate How Much Disk Space You Wasted
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Its not the new term for DBAs. MySQL has an awesome parameter <code class="highlighter-rouge">innodb-file-per-tables</code> allows MySQL to create separate files for each tables. This helped a lot to manage the disk space in more efficient way. But when we perform a large batch job for delete or update the data in MySQL tables, you may face this fragmentation issue. Comparing with SQL server, MySQL’s fragmentation is not high. I had a similar situation where my Disk space was consuming 80% and when I check the huge files in OS, one table’s <code class="highlighter-rouge">idb</code> file consumed 300GB+. I know it has some wasted blocks(but not actually wasted, MySQL will use this space, it’ll not return this to OS) Then I checked the <code class="highlighter-rouge">information schema</code> to find out the data size and its index size. It was 27GB only. Then I realize, we did a batch operation to delete many billions of records in that table.</p>

        <div class="more-link">
          <a href="/mysql-calculate-how-much-disk-space-you-wasted/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-10-21T10:43:00+05:30">October 21, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/backfill-failed-delivery-from-kinesis-to-redshift-with-lambda/" rel="bookmark">BackFill Failed Delivery From Kinesis To RedShift With Lambda
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>If you are dealing with the realtime data stream from Kinesis to RedShift, then you may face this situation where Redshift was down due to some maintenance activity and kinesis firehose was not able to ingest the data. But it has awesome features to retry after the next 60 Minutes. I had a situation that, there was a password change on the RedShift cluster on staging infra and we didn’t notice that the data load was failing. Now we need to backfill all those data into redshift using manifest files. One simple way, list all the files in errors manifest folders and generate the copy command and run those commands in loop or import as a <code class="highlighter-rouge">.sql</code> file. But I wanted to automate this process with lambda.</p>

        <div class="more-link">
          <a href="/backfill-failed-delivery-from-kinesis-to-redshift-with-lambda/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-10-17T21:56:00+05:30">October 17, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/aws-glue-custom-output-file-size-and-fixed-number-of-files/" rel="bookmark">AWS Glue Custom Output File Size And Fixed Number Of Files
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>AWS Glue is the serverless version of EMR clusters. Many organizations now adopted to use Glue for their day to day BigData workloads. I have written a blog in Searce’s Medium publication for Converting the CSV/JSON files to parquet using AWS Glue. Till now its many people are reading that and implementing on their infra. But many people are commenting about the Glue is producing a huge number for output files(converted Parquet files) in S3, even for converting 100MB of CSV file will produce 500+ Parquet files. we need to customize this output file size and number of files.</p>

        <div class="more-link">
          <a href="/aws-glue-custom-output-file-size-and-fixed-number-of-files/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-10-08T02:10:00+05:30">October 8, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/redshift-unload-all-tables-to-s3/" rel="bookmark">RedShift Unload All Tables To S3
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>RedShift <code class="highlighter-rouge">unload</code> function will help us to export/unload the data from the tables to S3 directly. It actually runs a select query to get the results and them store them into S3. But unfortunately, it supports only one table at a time. You need to create a script to get the all the tables then store it in a variable, and loop the unload query with the list of tables. Here I have done with PL/SQL way to handle this. You can Export/Unload all the tables to S3 with partitions.</p>

        <div class="more-link">
          <a href="/redshift-unload-all-tables-to-s3/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-10-07T01:10:00+05:30">October 7, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/how-gcp-browser-bases-ssh-works/" rel="bookmark">How GCP Browser Based SSH Works
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>If you are using GCP platform and lazy to setup VPN or bastion host, then you may familiar with using SSH connection via a browser. Yeah, its just one click to login to the Linux VM. Here are some questions for you.</p>

        <div class="more-link">
          <a href="/how-gcp-browser-bases-ssh-works/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-10-02T00:02:00+05:30">October 2, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/cloudwatch-custom-log-filter-alarm-for-kinesis-load-failed-event/" rel="bookmark">CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Kinesis Firehose is pushing the realtime data to S3, Redshift, ElasticSearch, and Splunk for realtime/Near real-time analytics. Sometime the target may not available due to maintenance or any reason. So the Kinesis will automatically push the data to S3 and create the manifest file in the <code class="highlighter-rouge">errors</code> directory. Then later we can reload the data into our targets. But unfortunately, there is no one-step action to set notification if the load is failed. In this blog, im writing how can we setup Cloudwatch custom log filter alarm for kinesis load failed events.</p>

        <div class="more-link">
          <a href="/cloudwatch-custom-log-filter-alarm-for-kinesis-load-failed-event/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-10-01T11:00:00+05:30">October 1, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/relationalize-unstructured-data-in-aws-athena-with-grokserde/" rel="bookmark">Relationalize Unstructured Data In AWS Athena with GrokSerDe
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Managing the logs in a centralized repository is one of the most common best practices in the DevOps world. Application logs, system logs, error logs, and any databases logs also will be pushed into your centralized repository. You can use ELK stack or Splunk to visualize the logs to get better insights about it. But as a SQL guy, I wanted to solve this problem with Bigdata ecosystem(use SQL). As a part of that process, we can relationalize unstructured data in AWS Athena with the help of GrokSerDe.</p>

        <div class="more-link">
          <a href="/relationalize-unstructured-data-in-aws-athena-with-grokserde/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-09-23T00:00:00+05:30">September 23, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/redshift-unload-to-s3-with-partitions-stored-procedure-way/" rel="bookmark">RedShift Unload to S3 With Partitions - Stored Procedure Way
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Redshift unload is the fastest way to export the data from Redshift cluster. In BigData world, generally people use the data in S3 for DataLake. So its important that we need to make sure the data in S3 should be partitioned. So we can use Athena, RedShift Spectrum or EMR External tables to access that data in an optimized way. If you are dealing with multiple tables, then you can loop the table names in a shell script or Python code. But as a SQL guy, I choose stored procedures to do this. It made export process simple.</p>

        <div class="more-link">
          <a href="/redshift-unload-to-s3-with-partitions-stored-procedure-way/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-08-27T09:47:00+05:30">August 27, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/mysql-convert-binlog-based-replication-to-gtid-replication-without-downtime/" rel="bookmark">MySQL Convert Binlog Based Replication To GTID Replication Without Downtime
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>This title may be suitable for the new age MySQL Users. Because in 5.7 onwards its already supported to enable GTID online. But still few of my mission critical databases are in 5.6 and handling <code class="highlighter-rouge">70k QPS</code>. So I know enabling GTID needs downtime for this. But in my case, the GTID has been already implemented. But still the replication is using Binlog file name and position for replicating the data.</p>

        <div class="more-link">
          <a href="/mysql-convert-binlog-based-replication-to-gtid-replication-without-downtime/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-08-24T13:28:00+05:30">August 24, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/mongodb-add-node-to-replica-set-without-initial-sync-in-gcp-aws/" rel="bookmark">MongoDB Add Node To Replica Set Without Initial Sync In GCP/AWS
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Adding a new node to the MongoDB replica set with huge amount of data will take a lot of time to perform the initial sync. Recently I was working on a replica set where I need to replace all the nodes in the existing shard and add a new node to the shard’s replica set. The data size is 3TB on each shard. The entire infra is on GCP. I planned to do this activity with native approach, but later I realized it won’t work as I expected.</p>

        <div class="more-link">
          <a href="/mongodb-add-node-to-replica-set-without-initial-sync-in-gcp-aws/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-08-20T00:00:00+05:30">August 20, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/database-mirroring-is-still-a-mystery/" rel="bookmark">Database Mirroring is still a Mystery
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><strong>Database Mirroring</strong> - A replication feature which supports automatic failover and supported in standard as well. During SQL server 2016, Microsoft announced that Mirror will be removed from further releases. But still, its there and Documentations are showing it’s going to deprecate soon. The Alwayson Availability Groups is the permanent High availability solution for SQL server.</p>

        <div class="more-link">
          <a href="/database-mirroring-is-still-a-mystery/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-08-08T10:00:00+05:30">August 8, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/monitor-cassandra-clusters-with-percona-pmm-jmx-grafana-and-prometheus/" rel="bookmark">Monitor Cassandra Clusters with Percona PMM - JMX Grafana and Prometheus
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>While reading this title you may think about what this guy is going to do? Its all about <a href="https://github.com/prometheus/jmx_exporter">JMX exporter</a> from prometheus and Grafana. Yes, its already implemented by many companies and Grafana has some cool dashboards. But as a DBA, in <a href="https://medium.com/Searce">Searce</a> we are managing many customers and all of them are using many types of databases. So from a DBA’s perspective to monitor  all databases in one place is always a great thing right. If you are a MySQL DBA, then you must have heard about <a href="https://www.percona.com/software/database-tools/percona-monitoring-and-management">PMM</a>. Its an awesome monitoring tool and its open source. Also it has Dashboards for Linux metrics, MongoDB and PostgreSQL.</p>

        <div class="more-link">
          <a href="/monitor-cassandra-clusters-with-percona-pmm-jmx-grafana-and-prometheus/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-05-17T00:00:00+05:30">May 17, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/mysql-pitr-the-fastest-way-with-devops/" rel="bookmark">MySQL PITR The Fastest Way With DevOps
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Point In Time Recovery - is a nightmare for DBAs if the MySQL clusters are self managed. It was 10PM, after had my dinner I was simply watching some shows in YouTube. And my phone was ringing, the customer on other side. Due to some bad queries, one of the main table get updated without where clause. Then suddenly everyone joined the call and asking me to bring the data back. That day it took 6 to 8 Hours to bring the data. Yes, every DBAs will do one or two biggest mistakes. In my carrier I would say this was that day. So here is my MySQL PITR the fastest way with DevOps.</p>

        <div class="more-link">
          <a href="/mysql-pitr-the-fastest-way-with-devops/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-03-04T00:00:00+05:30">March 4, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/mysql-exact-row-count-for-all-the-tables/" rel="bookmark">MySQL Exact Row Count For All The Tables
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/uploads/MySQL Exact Row Count For All The Tables.jpg" alt="MySQL Exact Row Count For All The Tables" title="MySQL Exact Row Count For All The Tables" /></p>

        <div class="more-link">
          <a href="/mysql-exact-row-count-for-all-the-tables/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-02-25T17:05:00+05:30">February 25, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/mysql-devops-automate-database-archive/" rel="bookmark">MySQL With DevOps 1 - Automate Database Archive
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/MySQL-With-DevOps-1-Automate-Database-Archive_cover-1024x398.jpg" alt="MySQL With DevOps 1 - Automate Database Archive" /></p>

        <div class="more-link">
          <a href="/mysql-devops-automate-database-archive/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-02-02T19:15:21+05:30">February 2, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/create-aurora-read-replica-aws-cli-lambda-python/" rel="bookmark">Create Aurora Read Replica With AWS CLI/Lambda Python
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="https://thedataguy.in/assets/Create-Aurora-Read-Replica-With-AWS-CLI-Lambda-Python.jpg" alt="Create Aurora Read Replica With AWS CLI-Lambda Python" title="Create Aurora Read Replica With AWS CLI-Lambda Python" /></p>

        <div class="more-link">
          <a href="/create-aurora-read-replica-aws-cli-lambda-python/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-01-26T00:39:11+05:30">January 26, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/aws-documentdb-a-nosql-equivalent-for-aurora/" rel="bookmark">AWS DocumentDB - A NoSQL Equivalent For Aurora
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/AWS-DocumentDB.jpg" alt="AWS DocumentDB" /></p>

        <div class="more-link">
          <a href="/aws-documentdb-a-nosql-equivalent-for-aurora/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-01-12T19:27:12+05:30">January 12, 2019
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/automation-script-for-percona-xtrabackup-full-incremental/" rel="bookmark">Automation Script For Percona Xtrabackup FULL/Incremental
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Automation-Script-For-Percona-Xtrabackup-FULLIncremental.jpg" alt="Automation Script For Percona Xtrabackup FULL/Incremental" /></p>

        <div class="more-link">
          <a href="/automation-script-for-percona-xtrabackup-full-incremental/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2019-01-01T21:03:18+05:30">January 1, 2019
  </time>
    </footer>
  
</article>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
  </section>

  <section id="2018" class="taxonomy-section">
    <h2 class="taxonomy-title">2018</h2>
    <div class="entries-list">
      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/encrypt-key-files-and-passwords-in-rundeck/" rel="bookmark">RunDeck Series 5 - Encrypt Key Files And Passwords In RunDeck
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Encrypt-Key-Files-And-Passwords_cover-1024x398.jpg" alt="Encrypt Key Files And Passwords" /></p>

        <div class="more-link">
          <a href="/encrypt-key-files-and-passwords-in-rundeck/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-12-29T16:16:53+05:30">December 29, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/configure-rundeck-smtp-with-aws-ses/" rel="bookmark">RunDeck Series 4 - Configure RunDeck SMTP With AWS SES
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Configure-RunDeck-SMTP-With-AWS-SES-1024x285.jpg" alt="Configure RunDeck SMTP With AWS SES" /></p>

        <div class="more-link">
          <a href="/configure-rundeck-smtp-with-aws-ses/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-12-29T12:44:00+05:30">December 29, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/configure-nginx-proxypass-for-rundeck/" rel="bookmark">RunDeck Series 3 - Configure Nginx ProxyPass For RunDeck
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Configure-ProxyPass-For-Rundeck-1024x316.jpg" alt="Configure ProxyPass For Rundeck" /></p>

        <div class="more-link">
          <a href="/configure-nginx-proxypass-for-rundeck/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-12-29T11:44:59+05:30">December 29, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/add-nodes-to-the-rundeck/" rel="bookmark">RunDeck Series 2 - Add Nodes to the Rundeck
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Add nodes to the Rundeck Server is very next step after installation. Here we are going to see adding Linux nodes to Rundeck. After Rudeck 3.0+ the resources.xml file will not create automatically. In previous versions, this file was automatically created while creating a project. We’ll node nodes details in this file. But in latest versions, it’s not creating automatically.</p>

        <div class="more-link">
          <a href="/add-nodes-to-the-rundeck/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-12-29T01:27:17+05:30">December 29, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/rundeck-install-configure-centos-with-mysql/" rel="bookmark">RunDeck Series 1 - Install And Configure RunDeck 3.0 On CentOS 7
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Rundeck-Series-Install-And-Configure-RunDeck-3.0-On-CentOS-7.png" alt="Install And Configure RunDeck 3.0 On CentOS 7" /></p>

        <div class="more-link">
          <a href="/rundeck-install-configure-centos-with-mysql/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-10-12T13:37:29+05:30">October 12, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/archive-mysql-data-in-chunks/" rel="bookmark">Archive MySQL Data In Chunks Using Stored Procedure
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>In a DBA’s day to day activities, we are doing Archive operation on our transnational database servers to improve your queries and control the Disk space. The archive is a most expensive operation since its involved a huge number of Read and Write will be performed. So its mandatory to run the archive queries in chunks. The archive is depended on business use. Many of us need a copy of the data on an archive database to refer later. To perform the archive we can just simply run the delete query with the limit. But we need to run the query again and again until the matched rows count is 0. We can create a procedure to do this in a while loop. I have created one such procedure to archive many tables. </p>

        <div class="more-link">
          <a href="/archive-mysql-data-in-chunks/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-09-26T18:42:25+05:30">September 26, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/internals-of-google-datastore-and-technical-overview/" rel="bookmark">Internals Of Google DataStore And Technical Overview
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Google Cloud Platform provides many varieties of solutions. In Data world particularly in NoSQL technology Google provides Datastore a highly scalable and availability solution with ACID capabilities. I have read a lot about the Google Datastore and its Internals. Here Im going to merge everything as a single blog post.</p>

        <div class="more-link">
          <a href="/internals-of-google-datastore-and-technical-overview/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-09-11T01:30:54+05:30">September 11, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/mysql-gtid-vs-mariadb-gtid/" rel="bookmark">MySQL GTID vs MariaDB GTID
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>MySQL supports three types for binlog format. For safer binlog based replication its recommended to use ROW based replication. But even though in some worst cases this leads to data inconsistency. Later MySQL came up with the concept of GTID (global transaction identifiers) which generates the unique binlog entries to avoid any data inconsistency. This feature supports in MySQL 5.6+. Percona MySQL Servers is also using the same structure of MySQL’s  GTID. But MariaDB GTID is bit different.</p>

        <div class="more-link">
          <a href="/mysql-gtid-vs-mariadb-gtid/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-08-24T01:18:53+05:30">August 24, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/convert-mysql-two-digit-year-to-four-digit-year/" rel="bookmark">How To Convert MySQL Two Digit Year To Four Digit Year
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/How-To-Convert-MySQL-Two-Digit-Year-To-Four-Digit-Year.png" alt="How To Convert MySQL Two Digit Year To Four Digit Year" /></p>

        <div class="more-link">
          <a href="/convert-mysql-two-digit-year-to-four-digit-year/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-07-20T20:57:29+05:30">July 20, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/automate-aws-redshift-snapshot-and-restore/" rel="bookmark">Automate AWS RedShift Snapshot And Restore
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Automate-AWS-RedShift-Snapshot-And-Restore.jpg" alt="Automate AWS RedShift Snapshot And Restore" /></p>

        <div class="more-link">
          <a href="/automate-aws-redshift-snapshot-and-restore/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-07-10T17:26:20+05:30">July 10, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/automatically-create-aws-athena-partitions-for-cloudtrail-between-two-dates/" rel="bookmark">AWS Athena Automatically Create Partition For Between Two Dates
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/AWS-Athena-Automatically-Create-Partition-For-Between-Two-Dates.jpg" alt="AWS Athena Automatically Create Partition For Between Two Dates" /></p>

        <div class="more-link">
          <a href="/automatically-create-aws-athena-partitions-for-cloudtrail-between-two-dates/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-05-30T15:44:16+05:30">May 30, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/automate-aws-athena-create-partition-on-daily-basis/" rel="bookmark">Automate AWS Athena Create Partition On Daily Basis
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Automate-AWS-Athena-Create-Partition-On-Daily-Basis.png" alt="Automate AWS Athena Create Partition On Daily Basis" /></p>

        <div class="more-link">
          <a href="/automate-aws-athena-create-partition-on-daily-basis/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-05-30T14:10:37+05:30">May 30, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/automatically-add-ec2-instances-to-ad/" rel="bookmark">Automatically Add EC2 Instances to Active Directory Domain
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>Windows Servers are in AWS will show some glitches in sometimes.<a href="http://www.sqlgossip.com/windows-server-2016-in-aws-unable-to-resolve-public-and-local-dns/"> My previous article</a>explains how Windows Server 2016 had some issues with DNS Suffix and Forwarders. This time I got a chance to play around with PowerShell automations. The requirement is automatically add EC2 instances to Active directory domain during the instance launch. It might be an On Demand purpose ec2 or launched by an Auto scaling group.</p>

        <div class="more-link">
          <a href="/automatically-add-ec2-instances-to-ad/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-05-15T18:24:42+05:30">May 15, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/migrate-postgresql-users-aws-rds-postgresql/" rel="bookmark">How To Migrate PostgreSQL Users To AWS RDS PostgreSQL
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p>How To Migrate PostgreSQL Users To AWS RDS PostgreSQL](/assets/How-To-Migrate-PostgreSQL-Users-To-AWS-RDS-PostgreSQL.jpg)</p>

        <div class="more-link">
          <a href="/migrate-postgresql-users-aws-rds-postgresql/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-03-06T12:21:18+05:30">March 6, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/windows-server-2016-in-aws-unable-to-resolve-public-and-local-dns/" rel="bookmark">Why Windows Server 2016 In AWS Unable To Resolve Public And Local DNS
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Windows-Server-2016-In-AWS-Unable-To-Resolve-Public-And-Local-DNS_Head.jpg" alt="Windows Server 2016 In AWS Unable To Resolve Public And Local DNS" /></p>

        <div class="more-link">
          <a href="/windows-server-2016-in-aws-unable-to-resolve-public-and-local-dns/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-03-02T23:28:24+05:30">March 2, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/automatically-enable-cdc-in-rds-sql-server/" rel="bookmark">Automatically Enable CDC In RDS SQL Server
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Automatically-Enable-CDC-In-RDS-SQL-Server.jpg" alt="Automatically Enable CDC In RDS SQL Server" /></p>

        <div class="more-link">
          <a href="/automatically-enable-cdc-in-rds-sql-server/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-02-10T17:58:23+05:30">February 10, 2018
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/dont-use-aws-ami-backup-ec2-database-server/" rel="bookmark">Don’t Use AWS AMI To Backup Your EC2 Database Server
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Dont-Use-AWS-AMI-To-Backup-Your-EC2-Database-Server.jpg" alt="Don't Use AWS AMI To Backup Your EC2 Database Server" /></p>

        <div class="more-link">
          <a href="/dont-use-aws-ami-backup-ec2-database-server/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2018-02-04T14:54:50+05:30">February 4, 2018
  </time>
    </footer>
  
</article>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
  </section>

  <section id="2017" class="taxonomy-section">
    <h2 class="taxonomy-title">2017</h2>
    <div class="entries-list">
      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/restore-corrupted-system-databases/" rel="bookmark">How To Restore Corrupted System Databases
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/How-To-Restore-Corrupted-System-Databases.jpg" alt="How To Restore Corrupted System Databases" /></p>

        <div class="more-link">
          <a href="/restore-corrupted-system-databases/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-12-31T19:25:03+05:30">December 31, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/configure-bucardo-replication-centos/" rel="bookmark">Configure Bucardo replication on CentOS
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/configure-bucardo-replication-centos.png" alt="configure bucardo replication centos" /></p>

        <div class="more-link">
          <a href="/configure-bucardo-replication-centos/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-12-28T18:51:28+05:30">December 28, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/postgrsql-insecure-directory-in-env-path/" rel="bookmark">Postgresql Insecure directory in ENV PATH - Unable To Start
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Postgresql-Unable-To-Start-Insecure-directory-in-ENV-PATH.jpg" alt="Postgrsql Insecure directory in env path" /></p>

        <div class="more-link">
          <a href="/postgrsql-insecure-directory-in-env-path/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-12-05T18:12:07+05:30">December 5, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/change-mysql-default-directories/" rel="bookmark">How To Change MySQL Default Data,Binlog,Error Log Directories
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/mysql-change-default-directories.jpg" alt="change mysql default data,binlog, error log directories" /></p>

        <div class="more-link">
          <a href="/change-mysql-default-directories/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-11-04T17:15:29+05:30">November 4, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/compare-two-sql-server-databases-using-tsql/" rel="bookmark">Compare Two SQL Server Databases using Tsql
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/Compare-Two-SQL-Server-Databases-using-Tsql.jpeg" alt="Compare Two SQL Server Databases using Tsql" /></p>

        <div class="more-link">
          <a href="/compare-two-sql-server-databases-using-tsql/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-10-19T23:01:30+05:30">October 19, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/install-oracle-12c-amazon-linux-silent-mode/" rel="bookmark">How To Install Oracle 12C on Amazon Linux In Silent Mode
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/How-To-Install-Oracle-12C-on-Amazon-Linux-In-Silent-Mode.jpeg" alt="How To Install Oracle 12C on Amazon Linux In Silent Mode" /></p>

        <div class="more-link">
          <a href="/install-oracle-12c-amazon-linux-silent-mode/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-10-19T22:48:22+05:30">October 19, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/aws-aurora-database-clone/" rel="bookmark">What Is AWS Aurora Database Clone
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/What-Is-AWS-Aurora-Database-Cloning.jpeg" alt="AWS Aurora Database Cloning" /></p>

        <div class="more-link">
          <a href="/aws-aurora-database-clone/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-10-19T22:16:31+05:30">October 19, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/sql-server-security-audit-script/" rel="bookmark">SQL Server All In One Security Audit Script
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/SQL-Server-All-In-One-Security-Audit-Script.jpeg" alt="All In One Security Audit Script" /></p>

        <div class="more-link">
          <a href="/sql-server-security-audit-script/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-10-19T22:01:19+05:30">October 19, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/map-linux-users-to-postgres-user/" rel="bookmark">How To Map Linux Users To Postgres User
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/How-to-map-Linux-users-to-PostgreSQL-user.jpeg" alt="linux users to postgres" title="linux user to postgres" /></p>

        <div class="more-link">
          <a href="/map-linux-users-to-postgres-user/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-10-19T21:49:05+05:30">October 19, 2017
  </time>
    </footer>
  
</article>

      
        

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/backup-with-dbatools-ola-hallengren/" rel="bookmark">SQLServer Backup with dbatools vs Olahallengren
</a>
      
    </h3>
    
  </header>
  
    <div class="entry-excerpt p-summary">
      
        <p><img src="/assets/SQLServer-Backup-with-dbatools-vs-Olahallengren.jpeg" alt="SQLServer Backup with dbatools vs Olahallengren" /></p>

        <div class="more-link">
          <a href="/backup-with-dbatools-ola-hallengren/">Read More</a>
        </div>
      
    </div>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2017-10-19T20:38:27+05:30">October 19, 2017
  </time>
    </footer>
  
</article>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
  </section>



        
      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="copyright">
    
      <p>&copy; 2020 Nitin Kumar Singh | Data Architect. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


  </body>

</html>

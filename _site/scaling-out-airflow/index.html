<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.1.1
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="http://localhost:4000/assets/airflow-image1.jpeg">
<script data-ad-client="ca-pub-9518508677792760" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<link rel="apple-touch-icon" sizes="57x57" href="/favicon.ico/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/favicon.ico/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/favicon.ico/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/favicon.ico/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/favicon.ico/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/favicon.ico/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/favicon.ico/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/favicon.ico/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="/favicon.ico/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon.ico/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico/favicon-16x16.png">
<link rel="manifest" href="/favicon.ico/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">  
  

  
    
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Setting up an Airflow Cluster | Nitin Kumar Singh</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Setting up an Airflow Cluster" />
<meta name="author" content="Nitin" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Airflow Cluster setup with Celery as an executor and PostgresSQL as the backend database." />
<meta property="og:description" content="Airflow Cluster setup with Celery as an executor and PostgresSQL as the backend database." />
<link rel="canonical" href="http://localhost:4000/scaling-out-airflow/" />
<meta property="og:url" content="http://localhost:4000/scaling-out-airflow/" />
<meta property="og:site_name" content="Nitin Kumar Singh" />
<meta property="og:image" content="http://localhost:4000/assets/airflow-image1.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-19T21:13:00+05:30" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/assets/airflow-image1.jpeg" />
<meta property="twitter:title" content="Setting up an Airflow Cluster" />
<meta name="twitter:site" content="@nitinkumarsin10" />
<meta name="twitter:creator" content="@https://twitter.com/nitinkumarsin10" />
<script type="application/ld+json">
{"image":"http://localhost:4000/assets/airflow-image1.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/scaling-out-airflow/"},"@type":"BlogPosting","url":"http://localhost:4000/scaling-out-airflow/","dateModified":"2019-12-19T21:13:00+05:30","datePublished":"2019-12-19T21:13:00+05:30","headline":"Setting up an Airflow Cluster","author":{"@type":"Person","name":"Nitin"},"description":"Airflow Cluster setup with Celery as an executor and PostgresSQL as the backend database.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Nitin Kumar Singh" href="/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  <link rel="stylesheet" href="/assets/css/applause-button.css" />
   <script src="/assets/js/applause-button.js"></script>

</head>


  <body class="layout--post  setting-up-an-airflow-cluster">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/">Home</a></li><li><a href="/posts/">Posts</a></li><li><a href="/categories/">Categories</a></li><li><a href="/tags/">Tags</a></li><li><a href="/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
    
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    
  
  
  

  <div class="page-image">
    <img src="/assets/airflow-image1.jpeg" class="entry-feature-image u-photo" alt="Setting up an Airflow Cluster" style="margin-top: 0;">
    
  </div>


    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title p-name">Setting up an Airflow Cluster
</h1>
        

      </header>
      
      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/assets/nks1.png" class="author-avatar u-photo" alt="Nitin"><div class="author-info"><div class="author-name">
        <em>by</em> <span class="p-name">Nitin</span>
      </div><ul class="author-links"><li class="author-link">
            <a class="u-url" rel="me" href="https://linkedin.com/in/nitinnks"><i class="fas fa-link fa-lg" title="LinkedIn"></i></a>
          </li></ul>

<span class="read-time">10 min read</span>

    <time class="page-date dt-published" datetime="2019-12-19T21:13:00+05:30"><a class="u-url" href="">2019-12-19</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/categories/#airflow" title="Pages filed under Airflow">Airflow</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/tags/#airflow" title="Pages tagged airflow" rel="tag">airflow</a></li><li class="page-taxonomy"><a href="/tags/#postgresql" title="Pages tagged postgresql" rel="tag">postgresql</a></li><li class="page-taxonomy"><a href="/tags/#data-pipeline" title="Pages tagged data-pipeline" rel="tag">data-pipeline</a></li><li class="page-taxonomy"><a href="/tags/#aws" title="Pages tagged aws" rel="tag">aws</a></li><li class="page-taxonomy"><a href="/tags/#rabbitmq" title="Pages tagged rabbitmq" rel="tag">rabbitmq</a></li>
  </ul>


        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- ad-vertical -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4568423959519693"
     data-ad-slot="4154399056"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
        <br/>
        <br/>
        <applause-button style="width: 58px; height: 58px; text-align:right" color="black" multiclap="true">

      </div>

      <div class="page-content">
        <div class="e-content">

          <p>Data-driven companies often hinge their business intelligence and product development on the execution of complex data pipelines. These pipelines are often referred to as data workflows, a term that can be somewhat opaque in that workflows are not limited to one specific definition and do not perform a specific set of functions per se. To orchestrate these workflows there are lot of schedulers like oozie, Luigi, Azkaban and Airflow. This blog demonstrate the setup of one of these orchestrator i.e Airflow.</p>

<h2 id="a-shot-intro">A shot intro:</h2>

<p>There are many orchestrators which are there in the technology space but Airflow provides a slight edge if our requirement hinges on of the following:</p>

<ul>
  <li>No cron – With Airflows included scheduler we don’t need to rely on cron to schedule our DAG and only use one framework (not like Luigi)</li>
  <li>Code Bases – In Airflow all the workflows, dependencies, and scheduling are done in python code. Therefore, it is rather easy to build complex structures and extend the flows.</li>
  <li>Language – Python is a language somewhat natural to pick up and was available on our team.</li>
</ul>

<p>However setting up a production grade setup required some effort and this blog address the same.</p>

<h2 id="basic-tech-terms">Basic Tech Terms:</h2>

<ul>
  <li><strong>Metastore:</strong> Its a database which stores information regarding the state of tasks. Database updates are performed using an abstraction layer implemented in SQLAlchemy. This abstraction layer cleanly separates the function of the remaining components of Airflow from the database.</li>
  <li><strong>Executor</strong>: The Executor is a message queuing process that is tightly bound to the Scheduler and determines the worker processes that actually execute each scheduled task.</li>
  <li><strong>Scheduler:</strong> The Scheduler is a process that uses DAG definitions in conjunction with the state of tasks in the metadata database to decide which tasks need to be executed, as well as their execution priority. The Scheduler is generally run as a service.</li>
  <li><strong>Worker:</strong> These are the processes that actually execute the logic of tasks, and are determined by the Executor being used.</li>
</ul>

<h2 id="aws-architecture">AWS Architecture:</h2>

<p><img src="/assets/airflow-schematic.jpg" alt="" /></p>

<p>Airflow provides an option to utilize CeleryExecutor to execute tasks in distributed fashion. In this mode, we can run several servers each running multiple worker nodes to execute the tasks. This mode uses Celery along with a message queueing service RabbitMQ.The diagram show the interactivity between different component services i.e. Airflow(Webserver and Scheduler), Celery(Executor) and RabbitMQ and Metastore in an AWS Environment.</p>

<p>For simplicity of the blog, we will demonstrate the setup of a single node master server and a single node worker server. Below is the following details for the setup:</p>

<ul>
  <li>EC2 Master Node - Running Scheduler and Webserver</li>
  <li>EC2 Worker Node - Running Celery Executor and Workers</li>
  <li>RDS Metastore - Storing information about metadata and dag</li>
  <li>EC2 Rabbit MQ Nodes - Running RabbitMq broker</li>
</ul>

<h2 id="enviornment-prerequisite">Enviornment Prerequisite:</h2>

<ul>
  <li>Operating System: Ubuntu 16.04/Ubuntu 18.04 / Debian System</li>
  <li>Python Environment: Python 3.5x</li>
  <li>DataBase: PostgreSql v11.2 (RDS)</li>
</ul>

<p>Once the prerequisites are taken care of, we can proceed with the installation.</p>

<h2 id="installation">Installation:</h2>

<p>The first step of the setup is the to configure the RDS Postgres database for airflow.
For that we need to connect to RDS Database using using admin user.For the sake of simplicity we are using command line utility from one of the EC2 servers to connect to our RDS Server. For command line client installation for postgres database on debian system execute following commands to install and execute.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span> Client Installation
apt-get <span class="nt">-y</span> update 
apt-get <span class="nb">install </span>postgresql-client</code></pre></figure>

<p>Once the client is installed try to connect to the Database using the admin user.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span> Generating IAM Token
<span class="nb">export </span><span class="nv">RDSHOST</span><span class="o">=</span><span class="s2">"{host_name}"</span>
<span class="nb">export </span><span class="nv">PGPASSWORD</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>aws rds generate-db-auth-token <span class="nt">--hostname</span> <span class="nv">$RDSHOST</span> <span class="nt">--port</span> 5432 <span class="nt">--region</span><span class="o">{</span>region<span class="o">}</span> <span class="nt">--username</span> <span class="o">{</span>admin_user<span class="o">}</span> <span class="si">)</span><span class="s2">"</span>

<span class="nt">--</span> Connecting to the Database
psql <span class="s2">"host=hostName port=portNumber dbname=DBName user=userName -password"</span></code></pre></figure>

<p>sslmode and sslrootcert parameter is used when we are using SSL/TLS based connection. For more information refere <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.Connecting.AWSCLI.PostgreSQL.html">here</a>.</p>

<p>Once the connection is established with the database create a database named airflow which will act as a primary source where all the metadata,scheduler and other information will be stored by airflow.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">CREATE DATABASE airflow<span class="p">;</span>

CREATE USER <span class="o">{</span>DATABASE_USER<span class="o">}</span> WITH PASSWORD ‘<span class="o">{</span>DATABASE_USER_PASSWORD<span class="o">}</span>’<span class="p">;</span>
GRANT ALL PRIVILEGES ON DATABASE airflow TO <span class="o">{</span>DATABASE_USER<span class="o">}</span><span class="p">;</span>
GRANT CONNECT ON DATABASE airflow TO <span class="o">{</span>DATABASE_USER<span class="o">}</span><span class="p">;</span></code></pre></figure>

<p>Once the above step is done the next step is to setup rabbitMQ in one the EC2 server. To install it follow the steps defined below.</p>

<ol>
  <li>Login as root</li>
  <li>Install RabbitMQ Server</li>
  <li>Verify status</li>
  <li>Install RabbitMQ Web Interface</li>
</ol>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">apt-get <span class="nb">install </span>rabbitmq-server
rabbitmqctl status
rabbitmq-plugins <span class="nb">enable </span>rabbitmq_management</code></pre></figure>

<p>Enable and start rabbitMQ server and add users and permissions to it.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span> Enable rabbitMQ server
service rabbitmq-server <span class="nb">enable
</span>service rabbitmq-server start
service rabbitmq-server status

<span class="nt">--</span> Add Users and permissions
rabbitmqctl add_user <span class="o">{</span>RABBITMQ_USER<span class="o">}</span> <span class="o">{</span>RABBITMQ_USER_PASSWORD<span class="o">}</span>
rabbitmqctl set_user_tags <span class="o">{</span>RABBITMQ_USER<span class="o">}</span> administrator</code></pre></figure>

<p>Make Virtual Host and Set Permission for the host.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">rabbitmqctl add_vhost <span class="o">{</span>VIRTUALHOST_NAME<span class="o">}</span>
rabbitmqctl set_permissions <span class="nt">-p</span> <span class="o">{</span>VIRTUALHOST_NAME<span class="o">}</span>
<span class="o">{</span>RABBITMQ_USER<span class="o">}</span> “.<span class="k">*</span>” “.<span class="k">*</span>” “.<span class="k">*</span>”</code></pre></figure>

<p>Download the rabbitadmin utility</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">wget http://127.0.0.1:15672/cli/rabbitmqadmin
<span class="nb">chmod</span> +x rabbitmqadmin</code></pre></figure>

<p>One Final step is to make a queue.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">./rabbitmqadmin <span class="nb">declare </span>queue –username<span class="o">={</span>RABBITMQ_USER<span class="o">}</span> –password<span class="o">={</span>RABBITMQ_USER_PASSWORD<span class="o">}</span> –vhost<span class="o">={</span>VIRTUALHOST_NAME<span class="o">}</span> <span class="nv">name</span><span class="o">={</span>QUEUE_NAME<span class="o">}</span> <span class="nv">durable</span><span class="o">=</span><span class="nb">true</span></code></pre></figure>

<p>We can now access the RabbitMQ UI utility by hitting the public IP at port 15672.</p>

<p>If you someone want to setup multiple machines to work as a RabbitMQ Cluster you can refere <a href="https://www.rabbitmq.com/clustering.html">here</a>.</p>

<p>Now we have all the building blocks. The final step is to setup airflow using celery.</p>

<p>Setting up of Airflow Using Celery :-</p>

<ul>
  <li>Install required libraries and dependencies for airflow on each node i.e worker and master</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python3-pip
apt-get <span class="nb">install </span>python-dev python3-dev libsasl2-dev gcc
apt-get <span class="nb">install </span>libffi-dev
apt-get <span class="nb">install </span>libkrb5-dev

<span class="nt">--</span> To get the pip version 
which pip</code></pre></figure>

<ul>
  <li>Install airflow and celery on each of the machine.</li>
</ul>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">pip <span class="nb">install </span>pyamqp
pip <span class="nb">install </span>psycopg2
pip <span class="nb">install </span>apache-airflow[postgres,rabbitmq,celery]
airflow version


<span class="nt">--Celery</span> Installation 
pip <span class="nb">install </span><span class="nv">celery</span><span class="o">==</span>4.3.0</code></pre></figure>

<h2 id="configuration">Configuration:</h2>

<p>We need to configure Zookeeper and Kafaka properties, Edit the <code class="language-plaintext highlighter-rouge">/etc/kafka/zookeeper.properties</code> on all the kafka nodes</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span> On Node 1
<span class="nv">dataDir</span><span class="o">=</span>/var/lib/zookeeper
<span class="nv">clientPort</span><span class="o">=</span>2181
<span class="nv">maxClientCnxns</span><span class="o">=</span>0
server.1<span class="o">=</span>0.0.0.0:2888:3888
server.2<span class="o">=</span>172.31.38.158:2888:3888
server.3<span class="o">=</span>172.31.46.207:2888:3888
autopurge.snapRetainCount<span class="o">=</span>3
autopurge.purgeInterval<span class="o">=</span>24
<span class="nv">initLimit</span><span class="o">=</span>5
<span class="nv">syncLimit</span><span class="o">=</span>2

<span class="nt">--</span> On Node 2
<span class="nv">dataDir</span><span class="o">=</span>/var/lib/zookeeper
<span class="nv">clientPort</span><span class="o">=</span>2181
<span class="nv">maxClientCnxns</span><span class="o">=</span>0
server.1<span class="o">=</span>172.31.47.152:2888:3888
server.2<span class="o">=</span>0.0.0.0:2888:3888
server.3<span class="o">=</span>172.31.46.207:2888:3888
autopurge.snapRetainCount<span class="o">=</span>3
autopurge.purgeInterval<span class="o">=</span>24
<span class="nv">initLimit</span><span class="o">=</span>5
<span class="nv">syncLimit</span><span class="o">=</span>2

<span class="nt">--</span> On Node 3
<span class="nv">dataDir</span><span class="o">=</span>/var/lib/zookeeper
<span class="nv">clientPort</span><span class="o">=</span>2181
<span class="nv">maxClientCnxns</span><span class="o">=</span>0
server.1<span class="o">=</span>172.31.47.152:2888:3888
server.2<span class="o">=</span>172.31.38.158:2888:3888
server.3<span class="o">=</span>0.0.0.0:2888:3888
autopurge.snapRetainCount<span class="o">=</span>3
autopurge.purgeInterval<span class="o">=</span>24
<span class="nv">initLimit</span><span class="o">=</span>5
<span class="nv">syncLimit</span><span class="o">=</span>2</code></pre></figure>

<p>We need to assign a unique ID for all the Zookeeper nodes.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"> <span class="nt">--</span> On Node 1
 <span class="nb">echo</span> <span class="s2">"1"</span> <span class="o">&gt;</span> /var/lib/zookeeper/myid
 
 <span class="nt">--On</span> Node 2
 <span class="nb">echo</span> <span class="s2">"2"</span> <span class="o">&gt;</span> /var/lib/zookeeper/myid
 
 <span class="nt">--On</span> Node 3
 <span class="nb">echo</span> <span class="s2">"3"</span> <span class="o">&gt;</span> /var/lib/zookeeper/myid</code></pre></figure>

<p>Now we need to configure Kafka broker. So edit the <code class="language-plaintext highlighter-rouge">/etc/kafka/server.properties</code> on all the kafka nodes.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--On</span> Node 1
broker.id.generation.enable<span class="o">=</span><span class="nb">true
</span>delete.topic.enable<span class="o">=</span><span class="nb">true
</span><span class="nv">listeners</span><span class="o">=</span>PLAINTEXT://:9092
zookeeper.connect<span class="o">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181
log.dirs<span class="o">=</span>/kafkadata/kafka
log.retention.hours<span class="o">=</span>168
num.partitions<span class="o">=</span>1

<span class="nt">--On</span> Node 2
broker.id.generation.enable<span class="o">=</span><span class="nb">true
</span>delete.topic.enable<span class="o">=</span><span class="nb">true
</span><span class="nv">listeners</span><span class="o">=</span>PLAINTEXT://:9092
log.dirs<span class="o">=</span>/kafkadata/kafka
zookeeper.connect<span class="o">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181
log.retention.hours<span class="o">=</span>168
num.partitions<span class="o">=</span>1

<span class="nt">--</span> On Node 3
broker.id.generation.enable<span class="o">=</span><span class="nb">true
</span>delete.topic.enable<span class="o">=</span><span class="nb">true
</span><span class="nv">listeners</span><span class="o">=</span>PLAINTEXT://:9092
log.dirs<span class="o">=</span>/kafkadata/kafka
zookeeper.connect<span class="o">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181
num.partitions<span class="o">=</span>1
log.retention.hours<span class="o">=</span>168</code></pre></figure>

<p>The next step is optimizing the <code class="language-plaintext highlighter-rouge">Java JVM Heap</code> size, In many places kafka will go down due to the less heap size. So Im allocating 50% of the Memory to Heap. But make sure more Heap size also bad. Please refer some documentation to set this value for very heavy systems.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vi /usr/bin/kafka-server-start
<span class="nb">export </span><span class="nv">KAFKA_HEAP_OPTS</span><span class="o">=</span><span class="s2">"-Xmx2G -Xms2G"</span></code></pre></figure>

<p>The another major problem in the kafka system is the open file descriptors. So we need to allow the kafka to open at least up to 100000 files.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vi /etc/pam.d/common-session
session required pam_limits.so

vi /etc/security/limits.conf

<span class="k">*</span>                       soft    nofile          10000
<span class="k">*</span>                       hard    nofile          100000
cp-kafka                soft    nofile          10000
cp-kafka                hard    nofile          100000</code></pre></figure>

<p>Here the <code class="language-plaintext highlighter-rouge">cp-kafka</code> is the default user for the kafka process.</p>

<h3 id="create-kafka-data-dir">Create Kafka data dir:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">mkdir</span> <span class="nt">-p</span> /kafkadata/kafka
<span class="nb">chown</span> <span class="nt">-R</span> cp-kafka:confluent /kafkadata/kafka
chmode 710 /kafkadata/kafka</code></pre></figure>

<h3 id="start-the-kafka-cluster">Start the Kafka cluster:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>systemctl start confluent-zookeeper
<span class="nb">sudo </span>systemctl start confluent-kafka
<span class="nb">sudo </span>systemctl start confluent-schema-registry</code></pre></figure>

<p>Make sure the Kafka has to automatically starts after the Ec2 restart.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>systemctl <span class="nb">enable </span>confluent-zookeeper
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>confluent-kafka
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>confluent-schema-registry</code></pre></figure>

<p>Now our kafka cluster is ready. To check the list of system topics run the following command.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kafka-topics <span class="nt">--list</span> <span class="nt">--zookeeper</span> localhost:2181

__confluent.support.metrics</code></pre></figure>

<h2 id="setup-debezium">Setup Debezium:</h2>

<p>Install the confluent connector and debezium MySQL connector on all the producer nodes.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">apt-get update 
<span class="nb">sudo </span>apt-get <span class="nb">install </span>default-jre
 
wget <span class="nt">-qO</span> - https://packages.confluent.io/deb/5.3/archive.key | <span class="nb">sudo </span>apt-key add -
<span class="nb">sudo </span>add-apt-repository <span class="s2">"deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main"</span>
<span class="nb">sudo </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt-get <span class="nb">install </span>confluent-hub-client confluent-common confluent-kafka-connect-s3 confluent-kafka-2.12</code></pre></figure>

<h3 id="configuration-1">Configuration:</h3>

<p>Edit the <code class="language-plaintext highlighter-rouge">/etc/kafka/connect-distributed.properties</code> on all the producer nodes to make our producer will run on a distributed manner.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span> On all the connector nodes
bootstrap.servers<span class="o">=</span>172.31.47.152:9092,172.31.38.158:9092,172.31.46.207:9092
group.id<span class="o">=</span>debezium-cluster
plugin.path<span class="o">=</span>/usr/share/java,/usr/share/confluent-hub-components</code></pre></figure>

<h3 id="install-debezium-mysql-connector">Install Debezium MySQL Connector:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">confluent-hub <span class="nb">install </span>debezium/debezium-connector-mysql:latest</code></pre></figure>

<p>it’ll ask for making some changes just select <code class="language-plaintext highlighter-rouge">Y</code> for everything.</p>

<h3 id="run-the-distributed-connector-as-a-service">Run the distributed connector as a service:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">vi /lib/systemd/system/confluent-connect-distributed.service

<span class="o">[</span>Unit]
<span class="nv">Description</span><span class="o">=</span>Apache Kafka - connect-distributed
<span class="nv">Documentation</span><span class="o">=</span>http://docs.confluent.io/
<span class="nv">After</span><span class="o">=</span>network.target

<span class="o">[</span>Service]
<span class="nv">Type</span><span class="o">=</span>simple
<span class="nv">User</span><span class="o">=</span>cp-kafka
<span class="nv">Group</span><span class="o">=</span>confluent
<span class="nv">ExecStart</span><span class="o">=</span>/usr/bin/connect-distributed /etc/kafka/connect-distributed.properties
<span class="nv">TimeoutStopSec</span><span class="o">=</span>180
<span class="nv">Restart</span><span class="o">=</span>no

<span class="o">[</span>Install]
<span class="nv">WantedBy</span><span class="o">=</span>multi-user.target</code></pre></figure>

<h3 id="start-the-service">Start the Service:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">systemctl <span class="nb">enable </span>confluent-connect-distributed
systemctl start confluent-connect-distributed</code></pre></figure>

<h2 id="configure-debezium-mysql-connector">Configure Debezium MySQL Connector:</h2>

<p>Create a <code class="language-plaintext highlighter-rouge">mysql.json</code> file which contains the MySQL information and other formatting options.</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
	</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mysql-connector-db01"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mysql-connector-db01"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"connector.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.debezium.connector.mysql.MySqlConnector"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.server.id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"tasks.max"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.history.kafka.bootstrap.servers"</span><span class="p">:</span><span class="w"> </span><span class="s2">"172.31.47.152:9092,172.31.38.158:9092,172.31.46.207:9092"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.history.kafka.topic"</span><span class="p">:</span><span class="w"> </span><span class="s2">"schema-changes.mysql"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.server.name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mysql-db01"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.hostname"</span><span class="p">:</span><span class="w"> </span><span class="s2">"172.31.84.129"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.port"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3306"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.user"</span><span class="p">:</span><span class="w"> </span><span class="s2">"bhuvi"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.password"</span><span class="p">:</span><span class="w"> </span><span class="s2">"my_stong_password"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"database.whitelist"</span><span class="p">:</span><span class="w"> </span><span class="s2">"proddb,test"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.key.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"key.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.key.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.value.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"value.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.value.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"value.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"key.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"transforms"</span><span class="p">:</span><span class="w"> </span><span class="s2">"unwrap"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"transforms.unwrap.type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.debezium.transforms.ExtractNewRecordState"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"transforms.unwrap.add.source.fields"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ts_ms"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"tombstones.on.delete"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
	</span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<ul>
  <li>“database.history.kafka.bootstrap.servers” - Kafka Servers IP.</li>
  <li>“database.whitelist” - List of databases to get the CDC.</li>
  <li>key.converter and value.converter and transforms parameters - By default Debezium output will have more detailed information. But I don’t want all of those information. Im only interested in to get the new row and the timestamp when its inserted.</li>
</ul>

<p>If you don’t want to customize anythings then just remove everything after the <code class="language-plaintext highlighter-rouge">database.whitelist</code></p>

<h3 id="register-the-mysql-connector">Register the MySQL Connector:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl <span class="nt">-X</span> POST <span class="nt">-H</span> <span class="s2">"Accept: application/json"</span> <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> http://localhost:8083/connectors <span class="nt">-d</span> @mysql.json</code></pre></figure>

<h3 id="check-the-status">Check the status:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl GET localhost:8083/connectors/mysql-connector-db01/status
<span class="o">{</span>
  <span class="s2">"name"</span>: <span class="s2">"mysql-connector-db01"</span>,
  <span class="s2">"connector"</span>: <span class="o">{</span>
    <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
    <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
  <span class="o">}</span>,
  <span class="s2">"tasks"</span>: <span class="o">[</span>
    <span class="o">{</span>
      <span class="s2">"id"</span>: 0,
      <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
      <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
    <span class="o">}</span>
  <span class="o">]</span>,
  <span class="s2">"type"</span>: <span class="s2">"source"</span>
<span class="o">}</span></code></pre></figure>

<h3 id="test-the-mysql-consumer">Test the MySQL Consumer:</h3>

<p>Now insert something into any tables in <code class="language-plaintext highlighter-rouge">proddb or test</code> (because we have whilelisted only these databaes to capture the CDC.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="n">use</span> <span class="n">test</span><span class="p">;</span>
<span class="k">create</span> <span class="k">table</span> <span class="n">rohi</span> <span class="p">(</span><span class="n">id</span> <span class="nb">int</span><span class="p">,</span>
<span class="n">fn</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
<span class="n">ln</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
<span class="n">phone</span> <span class="nb">int</span> <span class="p">);</span>

<span class="k">insert</span> <span class="k">into</span> <span class="n">rohi</span> <span class="k">values</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s1">'rohit'</span><span class="p">,</span> <span class="s1">'ayare'</span><span class="p">,</span><span class="s1">'87611'</span><span class="p">);</span></code></pre></figure>

<p>We can get these values from the Kafker brokers. Open any one the kafka node and run the below command.</p>

<p>I prefer confluent cli for this. By default it’ll not be available, so download manually.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl <span class="nt">-L</span> https://cnfl.io/cli | sh <span class="nt">-s</span> <span class="nt">--</span> <span class="nt">-b</span> /usr/bin/</code></pre></figure>

<h3 id="listen-the-below-topic">Listen the below topic:</h3>

<blockquote>
  <p><strong>mysql-db01.test.rohi</strong> <br />
This is the combination of <code class="language-plaintext highlighter-rouge">servername.databasename.tablename</code> <br />
servername(you mentioned this in as a server name in mysql json file).</p>
</blockquote>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">confluent <span class="nb">local </span>consume mysql-db01.test.rohi

<span class="nt">----</span>
The <span class="nb">local </span>commands are intended <span class="k">for </span>a single-node development environment
only, NOT <span class="k">for </span>production usage. https://docs.confluent.io/current/cli/index.html
<span class="nt">-----</span>

<span class="o">{</span><span class="s2">"id"</span>:1,<span class="s2">"fn"</span>:<span class="s2">"rohit"</span>,<span class="s2">"ln"</span>:<span class="s2">"ayare"</span>,<span class="s2">"phone"</span>:87611,<span class="s2">"__ts_ms"</span>:1576757407000<span class="o">}</span></code></pre></figure>

<h2 id="setup-s3-sink-connector-in-all-producer-nodes">Setup S3 Sink connector in All Producer Nodes:</h2>

<p>I want to send this data to S3 bucket. So you must have an EC2 IAM role which has access to the target S3 bucket. Or install <code class="language-plaintext highlighter-rouge">awscli</code> and configure access and secret key(but its not recommended)</p>

<h3 id="install-s3-connector">Install S3 Connector:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">confluent-hub <span class="nb">install </span>confluentinc/kafka-connect-s3:latest</code></pre></figure>

<p>Create <code class="language-plaintext highlighter-rouge">s3.json</code> file.</p>

<figure class="highlight"><pre><code class="language-json" data-lang="json"><span class="p">{</span><span class="w">
	</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3-sink-db01"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"connector.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.confluent.connect.s3.S3SinkConnector"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"storage.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.confluent.connect.s3.storage.S3Storage"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"s3.bucket.name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"bhuvi-datalake"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3-sink-db01"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"tasks.max"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"s3.region"</span><span class="p">:</span><span class="w"> </span><span class="s2">"us-east-1"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"s3.part.size"</span><span class="p">:</span><span class="w"> </span><span class="s2">"5242880"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"s3.compression.type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"gzip"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"timezone"</span><span class="p">:</span><span class="w"> </span><span class="s2">"UTC"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"locale"</span><span class="p">:</span><span class="w"> </span><span class="s2">"en"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"flush.size"</span><span class="p">:</span><span class="w"> </span><span class="s2">"10000"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"rotate.interval.ms"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3600000"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"topics.regex"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mysql-db01.(.*)"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.key.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"key.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.key.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"format.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.confluent.connect.s3.format.json.JsonFormat"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.value.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"value.converter.schemas.enable"</span><span class="p">:</span><span class="w"> </span><span class="s2">"false"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"internal.value.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"value.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"key.converter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"org.apache.kafka.connect.json.JsonConverter"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"partitioner.class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"io.confluent.connect.storage.partitioner.HourlyPartitioner"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"path.format"</span><span class="p">:</span><span class="w"> </span><span class="s2">"YYYY/MM/dd/HH"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"partition.duration.ms"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3600000"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"rotate.schedule.interval.ms"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3600000"</span><span class="w">
	</span><span class="p">}</span><span class="w">
</span><span class="p">}</span></code></pre></figure>

<ul>
  <li><code class="language-plaintext highlighter-rouge">"topics.regex": "mysql-db01"</code> - It’ll send the data only from the topics which has <code class="language-plaintext highlighter-rouge">mysql-db01</code> as prefix. In our case all the MySQL databases related topics will start  with this prefix.</li>
  <li><code class="language-plaintext highlighter-rouge">"flush.size"</code> - The data will uploaded to S3 only after these many number of records stored. Or after <code class="language-plaintext highlighter-rouge">"rotate.schedule.interval.ms"</code> this duration.</li>
</ul>

<h3 id="register-this-s3-sink-connector">Register this S3 sink connector:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl <span class="nt">-X</span> POST <span class="nt">-H</span> <span class="s2">"Accept: application/json"</span> <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> http://localhost:8083/connectors <span class="nt">-d</span> @s3</code></pre></figure>

<h3 id="check-the-status-1">Check the Status:</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">curl GET localhost:8083/connectors/s3-sink-db01/status
<span class="o">{</span>
  <span class="s2">"name"</span>: <span class="s2">"s3-sink-db01"</span>,
  <span class="s2">"connector"</span>: <span class="o">{</span>
    <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
    <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
  <span class="o">}</span>,
  <span class="s2">"tasks"</span>: <span class="o">[</span>
    <span class="o">{</span>
      <span class="s2">"id"</span>: 0,
      <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
      <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
    <span class="o">}</span>,
    <span class="o">{</span>
      <span class="s2">"id"</span>: 1,
      <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
      <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
    <span class="o">}</span>,
    <span class="o">{</span>
      <span class="s2">"id"</span>: 2,
      <span class="s2">"state"</span>: <span class="s2">"RUNNING"</span>,
      <span class="s2">"worker_id"</span>: <span class="s2">"172.31.94.191:8083"</span>
    <span class="o">}</span>
  <span class="o">]</span>,
  <span class="s2">"type"</span>: <span class="s2">"sink"</span>
<span class="o">}</span></code></pre></figure>

<h3 id="test-the-s3-sync">Test the S3 sync:</h3>

<p>Insert the 10000 rows into the <code class="language-plaintext highlighter-rouge">rohi</code> table. Then check the S3 bucket. It’ll save the data in JSON format with GZIP compression. Also in a HOUR wise partitions.</p>

<p><img src="/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-1.jpg" alt="" /></p>

<p><img src="/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-2.jpg" alt="" /></p>

<h2 id="monitoring">Monitoring:</h2>
<p>Refer <a href="https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/">this post</a> to setup monitoring for MySQL Connector.</p>

<h2 id="more-tuning">More Tuning:</h2>

<ul>
  <li>Replication Factor is the other main parameter to the data durability.</li>
  <li>Use internal IP addresses as much as you can.</li>
  <li>By default debezium uses 1 Partition per topic. You can configure this based on your work load. But more partitions more through put needed.</li>
</ul>

<h2 id="references">References:</h2>

<ol>
  <li><a href="https://docs.confluent.io/current/kafka/deployment.html">Setup Kafka in production by confluent</a></li>
  <li><a href="https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/">How to choose number of partition</a></li>
  <li><a href="https://log-it.ro/2017/10/16/ubuntu-change-ulimit-kafka-not-ignore/">Open file descriptors for Kafka </a></li>
  <li><a href="https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-kafka-on-aws/">Kafka best practices in AWS</a></li>
  <li><a href="https://debezium.io/documentation/reference/1.0/tutorial.html">Debezium documentation</a></li>
  <li><a href="https://debezium.io/documentation/reference/1.0/configuration/event-flattening.html">Customize debezium output with SMT</a></li>
</ol>

<h3 id="debezium-series-blogs">Debezium Series blogs:</h3>

<ol>
  <li><a href="https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/">Build Production Grade Debezium Cluster With Confluent Kafka</a></li>
  <li><a href="https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/">Monitor Debezium MySQL Connector With Prometheus And Grafana</a></li>
  <li><a href="https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/">Debezium MySQL Snapshot From Read Replica With GTID</a></li>
  <li><a href="https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/">Debezium MySQL Snapshot From Read Replica And Resume From Master</a></li>
  <li><a href="https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot</a></li>
  <li><a href="https://medium.com/searce/realtime-cdc-from-mysql-using-aws-msk-with-debezium-28da5a4ca873">RealTime CDC From MySQL Using AWS MSK With Debezium</a></li>
</ol>

        </div>
        <hr>
        <h3> Share this article </h3>
      
        
          <div class="page-share">
  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fscaling-out-airflow%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--facebook btn--small"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i> <span>Share</span></a>
  <a href="https://twitter.com/intent/tweet?text=Setting+up+an+Airflow+Cluster%20http%3A%2F%2Flocalhost%3A4000%2Fscaling-out-airflow%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--twitter btn--small"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> <span>Tweet</span></a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fscaling-out-airflow%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--linkedin btn--small"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> <span>LinkedIn</span></a>
  <a href="https://reddit.com/submit?title=Setting+up+an+Airflow+Cluster&url=http%3A%2F%2Flocalhost%3A4000%2Fscaling-out-airflow%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--reddit btn--small"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i> <span>Reddit</span></a>
</div>

        

        
          

        

        <nav class="page-pagination" role="navigation">
  

  
</nav>

      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://twitter.com/nitinkumarsin10"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://github.com/nitinitleen1"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a></div><div class="copyright">
    
      <p>©2019 All Right Reserved.</p>

    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
